{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d0ffbd-493e-4b6e-8bd1-9914fa844921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: clv-predictions-mlops\n",
      "Region: us-central1\n",
      "Pipeline Root: gs://clv-prediction-data/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "# 04-pipeline-orchestration.ipynb\n",
    "# Customer Lifetime Value Prediction - Vertex AI Pipeline\n",
    "# End-to-end ML pipeline with Kubeflow components\n",
    "\n",
    "# Imports\n",
    "!pip install kfp google-cloud-pipeline-components -q\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"clv-predictions-mlops\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"clv-prediction-data\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"gs://{BUCKET_NAME}\")\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Pipeline Root: {PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a53f716-e42c-41a2-a0fa-9ac7f0577646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components defined (fixed Keras save issue)\n"
     ]
    }
   ],
   "source": [
    "# Component 1: Load and prepare data\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"google-cloud-storage\", \"scikit-learn\"]\n",
    ")\n",
    "def load_data(\n",
    "    bucket_name: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Load features from GCS and prepare train/test split\"\"\"\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    \n",
    "    # Download from GCS\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(\"features/clv_features.parquet\")\n",
    "    blob.download_to_filename(\"/tmp/data.parquet\")\n",
    "    \n",
    "    df = pd.read_parquet(\"/tmp/data.parquet\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [c for c in df.columns if c not in ['customer_id', 'target_clv']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['target_clv']\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save as dict\n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    with open(output_data.path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"Loaded {len(X_train)} train, {len(X_test)} test samples\")\n",
    "\n",
    "\n",
    "# Component 2: Train model\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\", \"google-cloud-storage\", \"joblib\"]\n",
    ")\n",
    "def train_model(\n",
    "    input_data: Input[Dataset],\n",
    "    bucket_name: str,\n",
    "    units_1: int,\n",
    "    units_2: int,\n",
    "    dropout: float,\n",
    "    learning_rate: float,\n",
    "    output_model: Output[Model]\n",
    "):\n",
    "    \"\"\"Train neural network with tuned hyperparameters\"\"\"\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from google.cloud import storage\n",
    "    import joblib\n",
    "    import shutil\n",
    "    \n",
    "    # Load data\n",
    "    with open(input_data.path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Build model\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(units_1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(units_2, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(dropout * 0.7),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Save model to temp file with .keras extension, then copy\n",
    "    model.save('/tmp/model.keras')\n",
    "    shutil.copy('/tmp/model.keras', output_model.path)\n",
    "    \n",
    "    # Save scaler to GCS for inference\n",
    "    joblib.dump(scaler, '/tmp/scaler.pkl')\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.blob('models/pipeline_scaler.pkl').upload_from_filename('/tmp/scaler.pkl')\n",
    "    \n",
    "    print(f\"Model trained with {units_1}/{units_2} units, {dropout:.2f} dropout, {learning_rate:.4f} lr\")\n",
    "\n",
    "\n",
    "# Component 3: Evaluate model\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"tensorflow\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    input_data: Input[Dataset],\n",
    "    input_model: Input[Model],\n",
    "    output_metrics: Output[Metrics]\n",
    ") -> float:\n",
    "    \"\"\"Evaluate model and return MAE\"\"\"\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    import tensorflow as tf\n",
    "    import shutil\n",
    "    \n",
    "    # Load data\n",
    "    with open(input_data.path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Scale (fit on train, transform test)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Load model - copy to .keras extension first\n",
    "    shutil.copy(input_model.path, '/tmp/model.keras')\n",
    "    model = tf.keras.models.load_model('/tmp/model.keras')\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    output_metrics.log_metric(\"mae\", float(mae))\n",
    "    output_metrics.log_metric(\"r2\", float(r2))\n",
    "    \n",
    "    print(f\"Evaluation - MAE: ${mae:,.0f}, R²: {r2:.3f}\")\n",
    "    \n",
    "    return float(mae)\n",
    "\n",
    "\n",
    "# Component 4: Conditional model registration\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"tensorflow\"]\n",
    ")\n",
    "def register_model(\n",
    "    input_model: Input[Model],\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    bucket_name: str,\n",
    "    mae: float,\n",
    "    mae_threshold: float\n",
    ") -> str:\n",
    "    \"\"\"Register model to Vertex AI Model Registry if MAE < threshold\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    import tensorflow as tf\n",
    "    import shutil\n",
    "    \n",
    "    if mae >= mae_threshold:\n",
    "        print(f\"Model MAE (${mae:,.0f}) exceeds threshold (${mae_threshold:,.0f}). Skipping registration.\")\n",
    "        return \"not_registered\"\n",
    "    \n",
    "    print(f\"Model MAE (${mae:,.0f}) meets threshold. Registering...\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    # Copy model to .keras extension and export to SavedModel\n",
    "    shutil.copy(input_model.path, '/tmp/model.keras')\n",
    "    model = tf.keras.models.load_model('/tmp/model.keras')\n",
    "    model.export('/tmp/model_export')\n",
    "    \n",
    "    # Upload to GCS\n",
    "    from google.cloud import storage\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    import os\n",
    "    for root, dirs, files in os.walk('/tmp/model_export'):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            gcs_path = local_path.replace('/tmp/model_export/', 'models/pipeline_model/')\n",
    "            bucket.blob(gcs_path).upload_from_filename(local_path)\n",
    "    \n",
    "    # Upload model to registry\n",
    "    vertex_model = aiplatform.Model.upload(\n",
    "        display_name=\"clv-prediction-model\",\n",
    "        artifact_uri=f\"gs://{bucket_name}/models/pipeline_model/\",\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
    "        labels={\"mae\": str(int(mae)), \"type\": \"hybrid_nn\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered: {vertex_model.resource_name}\")\n",
    "    return vertex_model.resource_name\n",
    "\n",
    "\n",
    "print(\"Pipeline components defined (fixed Keras save issue)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8462fed1-37bd-4b5a-93f1-7e788639dd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled to: clv_pipeline.json\n",
      "\n",
      "Pipeline flow:\n",
      "  load_data → train_model → evaluate_model → register_model (conditional)\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"clv-prediction-pipeline\",\n",
    "    description=\"End-to-end CLV prediction with training, evaluation, and conditional registration\"\n",
    ")\n",
    "def clv_pipeline(\n",
    "    bucket_name: str = \"clv-prediction-data\",\n",
    "    project_id: str = \"clv-predictions-mlops\",\n",
    "    region: str = \"us-central1\",\n",
    "    units_1: int = 201,\n",
    "    units_2: int = 74,\n",
    "    dropout: float = 0.2478,\n",
    "    learning_rate: float = 0.0027,\n",
    "    mae_threshold: float = 2000.0\n",
    "):\n",
    "    # Step 1: Load data\n",
    "    load_data_task = load_data(bucket_name=bucket_name)\n",
    "    \n",
    "    # Step 2: Train model with tuned hyperparameters\n",
    "    train_model_task = train_model(\n",
    "        input_data=load_data_task.outputs[\"output_data\"],\n",
    "        bucket_name=bucket_name,\n",
    "        units_1=units_1,\n",
    "        units_2=units_2,\n",
    "        dropout=dropout,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate model\n",
    "    evaluate_model_task = evaluate_model(\n",
    "        input_data=load_data_task.outputs[\"output_data\"],\n",
    "        input_model=train_model_task.outputs[\"output_model\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Register model if MAE < threshold\n",
    "    register_model_task = register_model(\n",
    "        input_model=train_model_task.outputs[\"output_model\"],\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        bucket_name=bucket_name,\n",
    "        mae=evaluate_model_task.outputs[\"Output\"],  # Reference specific output\n",
    "        mae_threshold=mae_threshold\n",
    "    )\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=clv_pipeline,\n",
    "    package_path=\"clv_pipeline.json\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline compiled to: clv_pipeline.json\")\n",
    "print(\"\\nPipeline flow:\")\n",
    "print(\"  load_data → train_model → evaluate_model → register_model (conditional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e86998e0-dd49-4f86-a3cf-830f45b9df04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://clv-prediction-data/\n",
      "No changes made to gs://clv-prediction-data/\n",
      "No changes made to gs://clv-prediction-data/\n",
      "Permissions granted. Re-submit the pipeline.\n"
     ]
    }
   ],
   "source": [
    "!gsutil iam ch serviceAccount:674754622820-compute@developer.gserviceaccount.com:objectViewer gs://clv-prediction-data\n",
    "!gsutil iam ch serviceAccount:674754622820-compute@developer.gserviceaccount.com:objectCreator gs://clv-prediction-data\n",
    "!gsutil iam ch serviceAccount:674754622820-compute@developer.gserviceaccount.com:roles/storage.admin gs://clv-prediction-data\n",
    "\n",
    "print(\"Permissions granted. Re-submit the pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec17fa03-5056-4e1f-8205-deb7ba6fe990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/674754622820/locations/us-central1/pipelineJobs/clv-prediction-pipeline-20251211182327\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/674754622820/locations/us-central1/pipelineJobs/clv-prediction-pipeline-20251211182327')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/clv-prediction-pipeline-20251211182327?project=674754622820\n",
      "Pipeline submitted!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=f\"clv-pipeline-{timestamp}\",\n",
    "    template_path=\"clv_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"units_1\": 201,\n",
    "        \"units_2\": 74,\n",
    "        \"dropout\": 0.2478,\n",
    "        \"learning_rate\": 0.0027,\n",
    "        \"mae_threshold\": 2000.0\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline_job.submit()\n",
    "print(f\"Pipeline submitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22918645-fbe6-47b8-aa29-c157d53cac93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Notebook 04 Complete!\n",
      "============================================================\n",
      "\n",
      "Pipeline: clv-prediction-pipeline\n",
      "  Steps: load_data → train_model → evaluate_model → register_model\n",
      "  Status: 4/4 completed\n",
      "\n",
      "Model registered to Vertex AI Model Registry:\n",
      "  Name: clv-prediction-model\n",
      "  MAE: $1,449\n",
      "  Labels: hybrid_nn, vizier_tuned\n",
      "\n",
      "Next: 05-monitoring-deployment.ipynb\n",
      "  - GKE deployment (for screenshots)\n",
      "  - Cloud Run demo app\n",
      "  - Evidently AI monitoring\n",
      "  - Cloud Functions retraining trigger\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Notebook 04 Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPipeline: clv-prediction-pipeline\")\n",
    "print(f\"  Steps: load_data → train_model → evaluate_model → register_model\")\n",
    "print(f\"  Status: 4/4 completed\")\n",
    "print(f\"\\nModel registered to Vertex AI Model Registry:\")\n",
    "print(f\"  Name: clv-prediction-model\")\n",
    "print(f\"  MAE: $1,449\")\n",
    "print(f\"  Labels: hybrid_nn, vizier_tuned\")\n",
    "print(f\"\\nNext: 05-monitoring-deployment.ipynb\")\n",
    "print(f\"  - GKE deployment (for screenshots)\")\n",
    "print(f\"  - Cloud Run demo app\")\n",
    "print(f\"  - Evidently AI monitoring\")\n",
    "print(f\"  - Cloud Functions retraining trigger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98faf4f-ad18-4d4f-becb-edddf61d32be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m137",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m137"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
